# -*- coding: utf-8 -*-
"""
Created on Mon Jan 20 18:37:45 2020

@author: vm_00
"""

import numpy as np
import pandas as pd
from tqdm import tqdm
from keras.models import Sequential
from keras.layers import Dense,Dropout,Activation,Flatten
from keras.optimizers import Adam
from collections import deque
import random
import classifiers_malware_detection as CF
import tensorflow as tf

FEATURES_N=10#Final number of features after feature reduction
MAX_STEPS=7#Denotes the maximum no of bit changes agent can make to permission vector of a file
UPDATE_TARGET_EVERY=5 #Target model is updated after every 5 episodes

EPISODES=10_000
epsilon=0.75
EPSILON_DECAY=0.99975
MIN_EPSILON=0.001
MINIBATCH_SIZE=64
DISCOUNT=0.99
REPLAY_MEMORY_SIZE = 50_000
MIN_REPLAY_MEMORY_SIZE = 1_000

AGGREGATE_STATS_EVERY=50

# tf.debugging.set_log_device_placement(True)

#with tf.device(:gpu:0)
class environment:
    #self.current_state will be an array of the FEATURES_N number of most important features
    #the action will be the index of the feature value that the agent tries to invert
    
    current_state=[]
    def __init__(self,n_actions):
        x=np.random.randint(0,len(CF.malwares.values))       
        self.current_state=np.array(CF.malwares.iloc[x][CF.imp_features])
        self.n_actions=n_actions
        
    def change_state(self,action):
        curr_state=self.current_state
        evaded=False
        if curr_state[action]==1:  #if action changes 1 to a 0
            new_state=curr_state
            reward=-100
            done=False
            
        else:
            new_state=curr_state
            new_state[action]=1
            
            y_pred=CF.clf_dt.predict(np.reshape(new_state,(-1,FEATURES_N)))
            if y_pred[0]==0:
                reward=100
                done=True
                evaded=True
                self.reset()
                return evaded,done,reward,new_state
            elif (CF.clf_dt.predict_proba(np.reshape(curr_state,(-1,FEATURES_N)))[0][0])<(CF.clf_dt.predict_proba(np.reshape(new_state,(-1,FEATURES_N)))[0][0]):
                reward=CF.clf_dt.predict_proba(np.reshape(new_state,(-1,FEATURES_N)))[0][0]-CF.clf_dt.predict_proba(np.reshape(curr_state,(-1,FEATURES_N)))[0][0]
                done=False
            else:
                reward=-1
                done=False
        self.current_state=new_state
        self.steps+=1
        if self.steps==MAX_STEPS:
            done=True
        
        
        return evaded,done,reward,new_state
                
    
    def reset(self):
        x=np.random.randint(0,len(CF.malwares.values))       
        self.current_state=np.array(CF.malwares.iloc[x][CF.imp_features])
        self.steps=0
        
        

class Agent:
    
    def create_model(self):
        model=Sequential()
        
        #model.add(Flatten())
        model.add(Dense(64,activation='relu',input_dim=FEATURES_N))#64
        model.add(Dropout(rate=0.3))
        model.add(Dense(32,activation='relu'))#64
        model.add(Dropout(rate=0.3))
        model.add(Dense(FEATURES_N))
        model.compile(loss="mse",optimizer=Adam(lr=0.01),metrics=['accuracy'])
        
        return model

    def __init__(self):
        self.model=self.create_model()
        
        self.target_model=self.create_model()
        self.target_model.set_weights(self.model.get_weights())
        
        self.replay_memory=deque(maxlen=REPLAY_MEMORY_SIZE)
        
        self.target_update_counter=0
        
    def update_replay_memory(self, transition):
        self.replay_memory.append(transition)
        
    def get_qs(self, state):
        return self.model.predict(np.array(state))[0]
    def train(self,terminal_state):
        if len(self.replay_memory)<MIN_REPLAY_MEMORY_SIZE:
            return
        
        minibatch=random.sample(self.replay_memory,MINIBATCH_SIZE)
        #transition format:state,action,reward,new_state,done
        
        curr_states=[]
        #curr_states=np.array(transition[0] for transition in minibatch)
        for transition in minibatch:
            curr_states.append(transition[0])
        curr_states=np.array(curr_states)
        #print(curr_states.shape)
        current_qs=self.model.predict(np.array(curr_states))
        new_states=[]
        for transition in minibatch:
            new_states.append(transition[3])
        curr_states=np.array(curr_states)
        
        future_qs=self.target_model.predict(np.array(new_states))
        
        
        for i,transition in enumerate(minibatch):
            
            if not transition[4]:
                max_q=np.max(future_qs[i])
                current_qs[i][transition[1]]=transition[2]+DISCOUNT*max_q
            else:
                current_qs[i][transition[1]]=transition[2]
                
        self.model.fit(np.array(curr_states),np.array(current_qs),batch_size=MINIBATCH_SIZE,shuffle=False,verbose=0)
        
        if terminal_state:
            self.target_update_counter += 1

        # If counter reaches set value, update target network with weights of main network
        if self.target_update_counter > UPDATE_TARGET_EVERY:
            self.target_model.set_weights(self.model.get_weights())
            self.target_update_counter = 0       
        
    def take_action1(self,curr):
        if np.random.random() > epsilon:
            # Get action from Q table
            action = np.argmax(self.get_qs([(curr)]))
        else:
            # Get random action
            action = np.random.randint(0, FEATURES_N)
        return action
      
    def take_action2(self,curr):
        action = np.argmax(self.get_qs([(curr)]))
        
        return action
    
agent=Agent()
env=environment(FEATURES_N)
#print(env.current_state)
#print(len(env.current_state))
for episodes in tqdm(range(EPISODES)):

    
        done=False

        episode_reward = 0
        
        env.reset()
        while not done:
            curr=env.current_state
            #print(curr)
            action=agent.take_action1(curr)
            evaded,done,reward,new_state=env.change_state(action)
            agent.update_replay_memory((curr,action,reward,new_state,done))
            agent.train(done)
            episode_reward += reward
            
          
    
        if epsilon > MIN_EPSILON:
            epsilon *= EPSILON_DECAY
            epsilon = max(MIN_EPSILON, epsilon)
            
            
            
file2='E:\\RL\\malware_dataset.csv'
malwares=pd.read_csv(file2,delimiter='\t')
malwares.drop(malwares.columns[[0,198]],axis=1,inplace=True)

evaded_n=0
for index,row in tqdm(malwares.iterrows()):
    done=False

    env.reset()
    env.current_state=list((row[(np.argsort(CF.fit.scores_)[-FEATURES_N:])]))
    
    while not done:
        curr=env.current_state
        action=agent.take_action1(curr)
        evaded,done,reward,new_state=env.change_state(action)
        agent.update_replay_memory((curr,action,reward,new_state,done))
        agent.train(done)
    if evaded:
        evaded_n+=1
        
print(evaded_n/malwares.shape[0])

